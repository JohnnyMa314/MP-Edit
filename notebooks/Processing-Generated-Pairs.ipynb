{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance Datasets Between Original Labels\n",
    "\n",
    "Sample **by MNLI index** from list of minimal pair edits of mask-filled sentence pairs until classes are balanced.\n",
    "\n",
    "With replacement **by MNLI index** and without replacement **by Generated Sentence Pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories1.png\n",
      "cleaned_100000_fine-tuned_content-words_gold-label_beam_MNLI_labeled.csv\n",
      "cleaned_100000_fine-tuned_data-slices_gold-label_beam_MNLI_labeled.csv\n",
      "cleaned_100000_fine-tuned_gradient_gold-label_beam_MNLI_labeled.csv\n",
      "instances1.png\n",
      "label-flips1.png\n",
      "test.png\n"
     ]
    }
   ],
   "source": [
    "!ls ../output/MNLI/mturk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['categories1.png',\n",
       " 'cleaned_100000_fine-tuned_content-words_gold-label_beam_MNLI_labeled.csv',\n",
       " 'cleaned_100000_fine-tuned_data-slices_gold-label_beam_MNLI_labeled.csv',\n",
       " 'cleaned_100000_fine-tuned_gradient_gold-label_beam_MNLI_labeled.csv',\n",
       " 'instances1.png',\n",
       " 'label-flips1.png',\n",
       " 'test.png']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = !ls ../output/MNLI/mturk\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_100000_fine-tuned_content-words_gold-label_beam_MNLI_labeled.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(66652, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "import pandas as pd\n",
    "import os\n",
    "from ast import literal_eval as make_tuple\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "# combine separate generations quickly\n",
    "# extension = 'csv'\n",
    "# all_filenames = [i for i in glob.glob('../'+ '*.{}'.format(extension))]\n",
    "# combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames])\n",
    "\n",
    "# df = combined_csv\n",
    "\n",
    "file_num = 1\n",
    "\n",
    "df = pd.read_csv('../output/MNLI/mturk/' + files[file_num])\n",
    "print(files[file_num])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66652, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete mask-fills where there is no new token\n",
    "df = df[df['mask-filled'] != df['hypothesis']]\n",
    "\n",
    "# cut examples that don't change tokens \n",
    "# same_token = [(make_tuple(token_pairs)[0] == make_tuple(token_pairs)[1]) for token_pairs in df['token_changes']]\n",
    "# df['same-token'] = same_token\n",
    "# df = df[df['same-token'] == False]\n",
    "\n",
    "# cut the examples that reached threshold\n",
    "#df = df[df['depth'] <= 9]\n",
    "\n",
    "# cut duplicate mask-filled sentences. only need one prem+hypo+hypo_mask for each pair.\n",
    "df = df.drop_duplicates(subset='mask-filled', keep=\"last\") \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for each data instance (original prem+hypo pair) gather top-k examples by similarity \n",
    "# topk = 5\n",
    "# print(len(df['line-num'].unique()))\n",
    "# df = df.groupby('line-num', as_index=False).apply(lambda x: x.nlargest(topk, 'Bert-Score'))\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Labels first\n",
    "df.loc[(df['label-changed'] == True) & (df['new-label-prob'] >= 0.8), 'flip-class'] = 'Certain-Flip'\n",
    "df.loc[(df['label-changed'] == True) & (df['new-label-prob'] < 0.8), 'flip-class'] = 'Uncertain-Flip'\n",
    "df.loc[(df['label-changed'] == False) & (df['new-label-prob'] >= 0.8), 'flip-class'] = 'Certain-Same'\n",
    "df.loc[(df['label-changed'] == False) & (df['new-label-prob'] < 0.8), 'flip-class'] = 'Uncertain-Same'\n",
    "\n",
    "# sort by Bert-Score and flip class\n",
    "df = df.sort_values(by=['flip-class','Bert-Score'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label \"interesting\" RTE data slices using slice function (SF) from Slice-based Learning/Polyjuice paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find \"interesting\" RTE data slices, add as filter\n",
    "# # use slices defined in paper. simple text matching.\n",
    "\n",
    "# temporal_prepositions = [\"after\", \"before\", \"past\"]\n",
    "# comparative_words = [\"more\", \"less\", \"better\", \"worse\", \"bigger\", \"smaller\"]\n",
    "# quantifiers = [\"all\", \"some\", \"none\"]\n",
    "# negation_words = [\"no\", \"not\", \"none\", \"no one\", \"nobody\", \"nothing\", \"neither\", \"nowhere\", \"never\", \"hardly\", \"scarcely\", \"barely\", \"doesnt\", \"isnt\", \"wasnt\", \"shouldnt\", \"wouldnt\", \"couldnt\", \"wont\", \"cant\", \"dont\"]\n",
    "\n",
    "# # if any slice words are in premise+hypothesis\n",
    "# total_SF = temporal_prepositions + comparative_words + quantifiers + negation_words\n",
    "\n",
    "# df['slice'] = (df['premise'] + df['hypothesis']).apply(lambda x: slice_function(total_SF, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputting\n",
    "df.to_csv('../output/MNLI/mturk/cleaned_' + files[file_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-f09ae2d45f02>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-f09ae2d45f02>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    good_linesEOF inside string starting at row\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# getting premises that have all 4 flip label category generations.\n",
    "import seaborn as sns\n",
    "\n",
    "df\n",
    "lens = []\n",
    "good_lines = pd.DataFrame()\n",
    "for line in df['line-num'].unique():\n",
    "    dt = df.loc[df['line-num'] == line]\n",
    "    lens.append(len(dt['flip-class'].unique()))\n",
    "    if len(dt['flip-class'].unique()) == 4:\n",
    "        good_lines = good_lines.append(dt)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "sns.histplot(lens)\n",
    "plt.xlabel('Number of Unique Flip Classes')\n",
    "plt.title('Instances by Generations Flip Classes: Content Words')\n",
    "plt.savefig('test.png', dpi = 100)\n",
    "\n",
    "good_linesEOF inside string starting at row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each Flip Class, get top 200 examples by Bert-Score, then sample 25 each.\n",
    "sample_df = pd.DataFrame()\n",
    "\n",
    "sample_df = sample_df.append(df[df['flip-class'] == 'Certain-Flip'].nlargest(200, 'Bert-Score').sample(25))\n",
    "sample_df = sample_df.append(df[df['flip-class'] == 'Uncertain-Flip'].nlargest(200, 'Bert-Score').sample(25))\n",
    "sample_df = sample_df.append(df[df['flip-class'] == 'Certain-Same'].nsmallest(200, 'Bert-Score').sample(25))\n",
    "sample_df = sample_df.append(df[df['flip-class'] == 'Uncertain-Same'].nsmallest(200, 'Bert-Score').sample(25))\n",
    "\n",
    "sample_df.to_csv('../output/MNLI/new/sample_' + files[file_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Sentence-Pair Classification Task\n",
    "\n",
    "Simply counting the direction of label flip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('contradiction', 'contradiction'): 903, ('neutral', 'neutral'): 820, ('entailment', 'entailment'): 445, ('contradiction', 'entailment'): 317, ('entailment', 'contradiction'): 124, ('contradiction', 'neutral'): 120, ('neutral', 'contradiction'): 78, ('neutral', 'entailment'): 54, ('entailment', 'neutral'): 37})\n"
     ]
    }
   ],
   "source": [
    "from ast import literal_eval as make_tuple\n",
    "from collections import Counter\n",
    "\n",
    "CS_data = df[df['label-changed'] == True]\n",
    "\n",
    "# word changes that shift label\n",
    "# words = [make_tuple(w1) for w1 in CS_data['token_changes']]\n",
    "# change_count = Counter(words)\n",
    "\n",
    "# count of masked words\n",
    "# filled_words = [make_tuple(w1)[1] for w1 in CS_data['token_changes']]\n",
    "# filled_count = Counter(filled_words)\n",
    "\n",
    "# count of label switches in contrast set\n",
    "switches = zip(df['orig-label'], df['new-label'])\n",
    "switch_changes = Counter(switches)\n",
    "print(switch_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'contradiction': 1105, 'neutral': 977, 'entailment': 816})"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(df['new-label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'contradiction': 1340, 'neutral': 952, 'entailment': 606})"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(df['orig-label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('contradiction', 'entailment'): 279, ('entailment', 'contradiction'): 109, ('contradiction', 'neutral'): 68, ('neutral', 'contradiction'): 65, ('neutral', 'entailment'): 32, ('entailment', 'neutral'): 19})\n"
     ]
    }
   ],
   "source": [
    "switches = zip(df.loc[df['flip-class'] == 'Certain-Flip', 'orig-label'], df.loc[df['flip-class'] == 'Certain-Flip', 'new-label'])\n",
    "switch_changes = Counter(switches)\n",
    "print(switch_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
