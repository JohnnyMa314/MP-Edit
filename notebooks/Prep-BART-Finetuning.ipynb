{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import jsonlines\n",
    "\n",
    "# adding special symbol to tokenizer\n",
    "EOS_re = re.compile(r'</s>')\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "def custom_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab, token_match=EOS_re.match)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.tokenizer = custom_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From MNLI code, read data line by line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nli_data(path):\n",
    "    \"\"\"\n",
    "    Load MultiNLI or SNLI data.\n",
    "    \"\"\"\n",
    "    LABEL_MAP = {\n",
    "    \"entailment\": 0,\n",
    "    \"neutral\": 1,\n",
    "    \"contradiction\": 2}\n",
    "    \n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            loaded_example = json.loads(line)\n",
    "            if loaded_example[\"gold_label\"] not in LABEL_MAP:\n",
    "                continue\n",
    "            loaded_example[\"label\"] = LABEL_MAP[loaded_example[\"gold_label\"]]\n",
    "            data.append(loaded_example)\n",
    "            \n",
    "        random.seed(1)\n",
    "        random.shuffle(data)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read MNLI data as JSONL. Split data in 50-50 train-test (finetune-generate). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_nli_data('multinli_1/multinli_1_train.jsonl')\n",
    "\n",
    "# train = pd.DataFrame(data[0:round(len(data)/2)])\n",
    "# test = pd.DataFrame(data[round(len(data)/2)+1:])\n",
    "\n",
    "# # output half of data to pre-train, half to generate over\n",
    "# train.to_json('mnli_bart_train.jsonl', orient ='records', lines=True)\n",
    "# test.to_json('mnli_bart_test.jsonl', orient ='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replicate the \"mask-infill\" training objective found in BART Denoising task. \n",
    "Identical masking to mask_fill objective. Mask hypothesis, then concate premise to masked hypothesis, separated by special token `</s>` used for sentence entailment fine-tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_hypo(hypo):\n",
    "    \n",
    "    sent = nlp(hypo)\n",
    "    # sample mask span length from poisson with lambda = 3, as in BART denoising paper\n",
    "    draw = random.poisson(3)\n",
    "    \n",
    "    # if masked span is longer than sentence\n",
    "    if len(sent) - draw <= 0:\n",
    "        return '<mask>'\n",
    "    \n",
    "    start_ind = random.randint(len(sent) - draw)\n",
    "    \n",
    "    # insert <mask> \n",
    "    if draw == 0:\n",
    "        return sent[:start_ind].text + ' <mask> ' + sent[start_ind:].text\n",
    "    \n",
    "    masked_sent = sent[:start_ind].text + ' <mask> ' + sent[start_ind + draw:].text\n",
    "    \n",
    "    return masked_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196351/196351 [30:02<00:00, 108.91it/s]\n"
     ]
    }
   ],
   "source": [
    "masked_pairs = []\n",
    "original_pairs = []\n",
    "for index, row in tqdm(train.iterrows(), total=train.shape[0]):\n",
    "    masked_hypo = mask_hypo(row['sentence2'])\n",
    "    \n",
    "    # appending masked and original premise + </s> + hypo pairs\n",
    "    masked_pairs.append(row['sentence1'] + ' </s> ' + mask_hypo(row['sentence2']))\n",
    "    original_pairs.append(row['sentence1'] + ' </s> ' + row['sentence2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(masked_pairs, open(\"denoised_input.pkl\", \"wb\"))\n",
    "pickle.dump(original_pairs, open(\"denoised_output.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the input-output of train-val-test splits for **fairseq** fine-tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "ip = pickle.load(open(\"denoised_input.pkl\", \"rb\"))\n",
    "op = pickle.load(open(\"denoised_output.pkl\", \"rb\"))\n",
    "\n",
    "ip = np.asarray(ip)\n",
    "op = np.asarray(op)\n",
    " \n",
    "indices = np.arange(ip.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "ip = ip[indices]\n",
    "op = op[indices]\n",
    "DIR = \"./\" #\"../Bart/Denoised/\"\n",
    "pickle.dump(indices, open(DIR+\"denoised_shuffle_indices.pkl\", \"wb\"))\n",
    "\n",
    "for i in range(len(ip)):\n",
    "    ip[i] = ' '.join(ip[i].split())\n",
    "    op[i] = ' '.join(op[i].split())\n",
    "\n",
    "with open(DIR+\"train.source\", \"w\") as f1, open(DIR+\"train.target\", \"w\") as f2:\n",
    "    for i in range(len(ip[:150000])):\n",
    "        f1.write(ip[i].strip()+'\\n')\n",
    "        f2.write(op[i].strip()+'\\n')\n",
    "\n",
    "\n",
    "with open(DIR+\"val.source\", \"w\") as f1, open(DIR+\"val.target\", \"w\") as f2:\n",
    "    for i in range(len(ip[150000:22000])):\n",
    "        f1.write(ip[21000+i].strip()+'\\n')\n",
    "        f2.write(op[21000+i].strip()+'\\n')\n",
    "\n",
    "with open(DIR+\"test.source\", \"w\") as f1, open(DIR+\"test.target\", \"w\") as f2:\n",
    "    for i in range(len(ip[22000:])):\n",
    "        f1.write(ip[22000+i].strip()+'\\n')\n",
    "        f2.write(op[22000+i].strip()+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2086/196351 [00:08<13:39, 236.98it/s]"
     ]
    }
   ],
   "source": [
    "lengths = [] \n",
    "for i in tqdm(range(len(train))):\n",
    "    lengths.append(len(nlp(train['sentence2'][i])))\n",
    "    \n",
    "plt.hist(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
